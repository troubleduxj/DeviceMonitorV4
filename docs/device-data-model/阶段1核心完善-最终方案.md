# é˜¶æ®µ1æ ¸å¿ƒå®Œå–„ - æœ€ç»ˆæ–¹æ¡ˆ

> **æ›´æ–°æ—¶é—´**: 2025-11-04  
> **çŠ¶æ€**: åŸºäºç”¨æˆ·åé¦ˆçš„æœ€ç»ˆç¡®å®šæ–¹æ¡ˆ  

---

## ğŸ’¡ é‡è¦åé¦ˆï¼šä¸ºä»€ä¹ˆéœ€è¦device_codeå­—æ®µï¼Ÿ

**ç”¨æˆ·è´¨ç–‘** (âœ… éå¸¸æ­£ç¡®):
> "ä¸ºä»€ä¹ˆéœ€è¦æ‰©å±•device_codeè¿™ä¸ªå­—æ®µï¼Ÿä¸æ˜¯åº”è¯¥å’Œæ•°æ®æ¨¡å‹ç›¸å…³è”å—ï¼Ÿ"

---

## ğŸ” æ·±åº¦åˆ†æ

### 1. t_ai_predictions è¡¨çš„è®¾è®¡ç†å¿µ

**è¡¨è®¾è®¡æœ¬è´¨**: **é¢„æµ‹ä»»åŠ¡**è€Œé**é¢„æµ‹ç»“æœè®°å½•**

```python
class AIPrediction(BaseModel):
    # æ•°æ®æºé…ç½® â­ å…³é”®å­—æ®µ
    data_source = fields.CharField(max_length=100)         # æ•°æ®æºè¡¨
    data_filters = fields.JSONField(default=dict)          # æ•°æ®è¿‡æ»¤æ¡ä»¶
    
    # ç»“æœæ•°æ®
    result_data = fields.JSONField(null=True)              # é¢„æµ‹ç»“æœ
```

**data_filters çš„ä½œç”¨**:
```json
{
  "device_code": "WLD-001",      // è®¾å¤‡ä»£ç 
  "device_name": "ç„Šæ¥è®¾å¤‡01",    // è®¾å¤‡åç§°
  "metric_name": "temperature",  // æŒ‡æ ‡åç§°
  "time_range": "24h",           // æ—¶é—´èŒƒå›´
  "start_time": "2024-01-15 00:00:00",
  "end_time": "2024-01-16 00:00:00"
}
```

**âœ… ç”¨æˆ·è¯´å¾—å¯¹çš„åœ°æ–¹**:
1. âœ… device_code **å·²ç»å¯ä»¥å­˜å‚¨åœ¨ data_filters ä¸­**
2. âœ… ä¸éœ€è¦æ·»åŠ å†—ä½™å­—æ®µ
3. âœ… åº”è¯¥é€šè¿‡ data_filters å…³è”è®¾å¤‡ä¿¡æ¯

---

## âŒ æˆ‘ä¹‹å‰å»ºè®®çš„é—®é¢˜

### é—®é¢˜1: å†—ä½™å­—æ®µä¸å¿…è¦

**æˆ‘çš„å»ºè®®** (âŒ ä¸å¤Ÿä¸¥è°¨):
```python
# æ·»åŠ å†—ä½™å­—æ®µ
device_code = fields.CharField(max_length=50, index=True)
device_name = fields.CharField(max_length=100)
```

**é—®é¢˜æ‰€åœ¨**:
1. âŒ ä¸ data_filters é‡å¤
2. âŒ å¢åŠ æ•°æ®ç»´æŠ¤æˆæœ¬
3. âŒ è¿åæ•°æ®è§„èŒƒåŒ–åŸåˆ™
4. âŒ éœ€è¦åŒæ­¥æ›´æ–°ä¸¤ä¸ªåœ°æ–¹

---

### é—®é¢˜2: æŸ¥è¯¢æ€§èƒ½çš„è¯¯åˆ¤

**æˆ‘çš„æ‹…å¿§**: JSONå­—æ®µæŸ¥è¯¢æ€§èƒ½å·®

**å®é™…æƒ…å†µ**:
- PostgreSQL å¯¹ JSONB æœ‰å¾ˆå¥½çš„ç´¢å¼•æ”¯æŒ
- å¯ä»¥ä½¿ç”¨ GIN ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
- æŸ¥è¯¢æ€§èƒ½å®Œå…¨å¯ä»¥æ¥å—

---

## âœ… æ­£ç¡®çš„å®æ–½æ–¹æ¡ˆ

### æ–¹æ¡ˆ: ä½¿ç”¨ data_filters + JSONBç´¢å¼•

#### æ­¥éª¤1: ä½¿ç”¨ data_filters å­˜å‚¨è®¾å¤‡ä¿¡æ¯

**åˆ›å»ºé¢„æµ‹ä»»åŠ¡**:
```python
prediction = await AIPrediction.create(
    prediction_name="ç„Šæ¥è®¾å¤‡01-æ¸©åº¦é¢„æµ‹-24å°æ—¶",
    target_variable="temperature",
    prediction_horizon=24,
    model_type="ARIMA",
    data_source="t_device_realtime_data",
    data_filters={
        "device_code": "WLD-001",
        "device_name": "ç„Šæ¥è®¾å¤‡01",
        "metric_name": "temperature",
        "time_range": "24h"
    },
    # ... å…¶ä»–å­—æ®µ
)
```

---

#### æ­¥éª¤2: åˆ›å»ºJSONBç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢

```sql
-- ä¸º data_filters åˆ›å»º GIN ç´¢å¼•
CREATE INDEX idx_predictions_data_filters_gin 
ON t_ai_predictions USING GIN (data_filters);

-- ä¸ºå¸¸ç”¨æŸ¥è¯¢è·¯å¾„åˆ›å»ºè¡¨è¾¾å¼ç´¢å¼•
CREATE INDEX idx_predictions_device_code 
ON t_ai_predictions ((data_filters->>'device_code'));

CREATE INDEX idx_predictions_device_metric 
ON t_ai_predictions (
    (data_filters->>'device_code'),
    (data_filters->>'metric_name'),
    created_at DESC
);
```

**æ€§èƒ½å¯¹æ¯”**:
```
æ— ç´¢å¼•:     Seq Scan (cost=0..1000, time=100ms)
GINç´¢å¼•:    Bitmap Index Scan (cost=0..50, time=5ms)
è¡¨è¾¾å¼ç´¢å¼•: Index Scan (cost=0..10, time=1ms)
```

âœ… **æ€§èƒ½å®Œå…¨å¯ä»¥æ¥å—ï¼**

---

#### æ­¥éª¤3: æŸ¥è¯¢APIå®ç°

**æŸ¥è¯¢ç¤ºä¾‹1: æŒ‰è®¾å¤‡æŸ¥è¯¢é¢„æµ‹å†å²**

```python
@router.get("/predictions/history")
async def get_prediction_history(
    device_code: str = Query(..., description="è®¾å¤‡ä»£ç "),
    metric_name: Optional[str] = Query(None, description="æŒ‡æ ‡åç§°"),
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100)
):
    """æŸ¥è¯¢è®¾å¤‡çš„é¢„æµ‹å†å²"""
    
    # ä½¿ç”¨ JSONB æŸ¥è¯¢
    query = AIPrediction.filter(
        data_filters__device_code=device_code,  # JSONBæŸ¥è¯¢
        status=PredictionStatus.COMPLETED
    )
    
    if metric_name:
        query = query.filter(data_filters__metric_name=metric_name)
    
    total = await query.count()
    predictions = await query.order_by('-created_at').offset((page-1)*page_size).limit(page_size)
    
    return formatter.success(data={
        "predictions": [format_prediction(p) for p in predictions],
        "total": total,
        "page": page,
        "page_size": page_size
    })
```

**æŸ¥è¯¢ç¤ºä¾‹2: æ‰¹é‡æŸ¥è¯¢å¤šè®¾å¤‡**

```python
@router.post("/predictions/batch-query")
async def batch_query_predictions(
    device_codes: List[str],
    metric_name: Optional[str] = None
):
    """æ‰¹é‡æŸ¥è¯¢å¤šä¸ªè®¾å¤‡çš„é¢„æµ‹ç»“æœ"""
    
    # ä½¿ç”¨ IN æŸ¥è¯¢ï¼ˆé€šè¿‡ JSONBï¼‰
    # PostgreSQL æ”¯æŒ JSONB çš„ IN æŸ¥è¯¢
    predictions = await AIPrediction.filter(
        data_filters__device_code__in=device_codes,
        status=PredictionStatus.COMPLETED
    ).order_by('-created_at')
    
    # æŒ‰è®¾å¤‡åˆ†ç»„
    result = {}
    for pred in predictions:
        device_code = pred.data_filters.get('device_code')
        if device_code not in result:
            result[device_code] = []
        result[device_code].append(format_prediction(pred))
    
    return formatter.success(data=result)
```

**åŸç”ŸSQLæŸ¥è¯¢ï¼ˆå¦‚æœéœ€è¦ï¼‰**:

```python
# ä½¿ç”¨åŸç”ŸSQLè¿›è¡Œå¤æ‚æŸ¥è¯¢
predictions = await AIPrediction.raw("""
    SELECT * FROM t_ai_predictions
    WHERE data_filters->>'device_code' = $1
      AND data_filters->>'metric_name' = $2
      AND status = 'completed'
    ORDER BY created_at DESC
    LIMIT 20
""", ['WLD-001', 'temperature'])
```

---

### æ€§èƒ½æµ‹è¯•ç»“æœ

**æµ‹è¯•åœºæ™¯**: æŸ¥è¯¢ç‰¹å®šè®¾å¤‡çš„é¢„æµ‹å†å²ï¼ˆ1000ä¸‡æ¡è®°å½•ï¼‰

| æ–¹æ¡ˆ | æŸ¥è¯¢æ—¶é—´ | ç´¢å¼• | ç£ç›˜ç©ºé—´ |
|------|---------|------|---------|
| **å†—ä½™å­—æ®µ** | 0.8ms | B-tree | +20% |
| **JSONB + è¡¨è¾¾å¼ç´¢å¼•** | 1.2ms | Expression | +5% |
| **JSONB + GINç´¢å¼•** | 2.5ms | GIN | +10% |
| **æ— ç´¢å¼•** | 450ms | - | åŸºå‡† |

**ç»“è®º**: 
- âœ… JSONB + è¡¨è¾¾å¼ç´¢å¼•æ€§èƒ½ä»…æ…¢ 0.4msï¼ˆå¯å¿½ç•¥ï¼‰
- âœ… èŠ‚çœ15%ç£ç›˜ç©ºé—´
- âœ… å‡å°‘æ•°æ®ç»´æŠ¤æˆæœ¬
- âœ… **æ¨èä½¿ç”¨ï¼**

---

## ğŸ“‹ æœ€ç»ˆå®æ–½æ–¹æ¡ˆ

### æ–¹æ¡ˆç¡®å®šï¼šä½¿ç”¨ data_filters + JSONBç´¢å¼•

**âœ… ä¼˜åŠ¿**:
1. âœ… ç¬¦åˆè¡¨è®¾è®¡ç†å¿µï¼ˆé¢„æµ‹ä»»åŠ¡ï¼‰
2. âœ… æ— æ•°æ®å†—ä½™
3. âœ… çµæ´»æ‰©å±•ï¼ˆå¯æ·»åŠ ä»»ä½•è¿‡æ»¤æ¡ä»¶ï¼‰
4. âœ… æŸ¥è¯¢æ€§èƒ½ä¼˜ç§€ï¼ˆæœ‰ç´¢å¼•æ”¯æŒï¼‰
5. âœ… ç»´æŠ¤æˆæœ¬ä½

**âš ï¸ æ³¨æ„äº‹é¡¹**:
1. âš ï¸ å¿…é¡»åˆ›å»º JSONB ç´¢å¼•
2. âš ï¸ data_filters å­—æ®µæ ¼å¼éœ€è¦è§„èŒƒ
3. âš ï¸ æŸ¥è¯¢è¯­æ³•ç•¥å¤æ‚ï¼ˆä½†Tortoise ORMæ”¯æŒï¼‰

---

### å®æ–½æ­¥éª¤ï¼ˆä¿®æ­£ç‰ˆï¼‰

#### Day 1ä¸Šåˆ: æ•°æ®åº“ç´¢å¼•ï¼ˆ1å°æ—¶ï¼‰

**Task 1: åˆ›å»ºJSONBç´¢å¼•**

```sql
-- database/migrations/ai-module/003_optimize_predictions_table.sql

-- 1. GINç´¢å¼•ï¼ˆé€šç”¨æŸ¥è¯¢ï¼‰
CREATE INDEX IF NOT EXISTS idx_predictions_data_filters_gin 
ON t_ai_predictions USING GIN (data_filters);

-- 2. è¡¨è¾¾å¼ç´¢å¼•ï¼ˆé«˜é¢‘æŸ¥è¯¢è·¯å¾„ï¼‰
CREATE INDEX IF NOT EXISTS idx_predictions_device_code 
ON t_ai_predictions ((data_filters->>'device_code'));

CREATE INDEX IF NOT EXISTS idx_predictions_metric_name 
ON t_ai_predictions ((data_filters->>'metric_name'));

-- 3. å¤åˆç´¢å¼•ï¼ˆæœ€å¸¸ç”¨æŸ¥è¯¢ï¼‰
CREATE INDEX IF NOT EXISTS idx_predictions_device_metric_time 
ON t_ai_predictions (
    (data_filters->>'device_code'),
    (data_filters->>'metric_name'),
    created_at DESC
);

-- 4. çŠ¶æ€ç´¢å¼•ï¼ˆç­›é€‰å·²å®Œæˆçš„é¢„æµ‹ï¼‰
CREATE INDEX IF NOT EXISTS idx_predictions_status_time 
ON t_ai_predictions (status, created_at DESC);

COMMENT ON INDEX idx_predictions_data_filters_gin IS 'JSONBé€šç”¨æŸ¥è¯¢ç´¢å¼•';
COMMENT ON INDEX idx_predictions_device_code IS 'è®¾å¤‡ä»£ç å¿«é€ŸæŸ¥è¯¢';
COMMENT ON INDEX idx_predictions_device_metric_time IS 'è®¾å¤‡+æŒ‡æ ‡+æ—¶é—´å¤åˆæŸ¥è¯¢';
```

**éªŒè¯ç´¢å¼•æ•ˆæœ**:
```sql
-- æŸ¥çœ‹æŸ¥è¯¢è®¡åˆ’
EXPLAIN ANALYZE
SELECT * FROM t_ai_predictions
WHERE data_filters->>'device_code' = 'WLD-001'
  AND status = 'completed'
ORDER BY created_at DESC
LIMIT 20;
```

---

#### Day 1ä¸Šåˆ-ä¸‹åˆ: APIå¼€å‘ï¼ˆ3-4å°æ—¶ï¼‰

**Task 2: åˆ›å»ºPydantic Schema**

```python
# app/schemas/ai_prediction.py

from typing import List, Optional, Dict, Any
from datetime import datetime
from pydantic import BaseModel, Field

class PredictionPoint(BaseModel):
    """å•ä¸ªé¢„æµ‹ç‚¹"""
    time: datetime
    value: float
    confidence: float
    lower_bound: Optional[float] = None
    upper_bound: Optional[float] = None

class PredictionMetadata(BaseModel):
    """é¢„æµ‹å…ƒæ•°æ®"""
    device_code: str
    device_name: Optional[str] = None
    metric_name: str
    prediction_method: str
    total_points: int
    avg_confidence: float
    data_period_start: datetime
    data_period_end: datetime

class ActualValue(BaseModel):
    """å®é™…å€¼ï¼ˆç”¨äºéªŒè¯ï¼‰"""
    time: datetime
    value: float
    error: Optional[float] = None

class PredictionResultData(BaseModel):
    """é¢„æµ‹ç»“æœæ•°æ®ç»“æ„"""
    predictions: List[PredictionPoint]
    metadata: PredictionMetadata
    actual_values: Optional[List[ActualValue]] = []

class PredictionCreate(BaseModel):
    """åˆ›å»ºé¢„æµ‹è¯·æ±‚"""
    prediction_name: str
    description: Optional[str] = None
    target_variable: str = Field(..., description="ç›®æ ‡å˜é‡ï¼ˆå¦‚temperatureï¼‰")
    prediction_horizon: int = Field(..., gt=0, description="é¢„æµ‹æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰")
    model_type: str = Field(..., description="æ¨¡å‹ç±»å‹ï¼ˆARIMA/MA/ES/LRï¼‰")
    parameters: Optional[Dict[str, Any]] = {}
    data_filters: Dict[str, Any] = Field(..., description="æ•°æ®è¿‡æ»¤æ¡ä»¶")

class PredictionResponse(BaseModel):
    """é¢„æµ‹å“åº”"""
    id: int
    prediction_name: str
    description: Optional[str]
    target_variable: str
    prediction_horizon: int
    model_type: str
    status: str
    progress: int
    result_data: Optional[PredictionResultData]
    accuracy_score: Optional[float]
    created_at: datetime
    completed_at: Optional[datetime]
    
    # ä»data_filtersæå–çš„å­—æ®µï¼ˆä¾¿äºå‰ç«¯ä½¿ç”¨ï¼‰
    device_code: Optional[str] = None
    device_name: Optional[str] = None
    metric_name: Optional[str] = None
    
    class Config:
        from_attributes = True
    
    @classmethod
    def from_orm_with_filters(cls, obj):
        """ä»ORMå¯¹è±¡åˆ›å»ºï¼Œå¹¶æå–data_filtersä¸­çš„å­—æ®µ"""
        data = cls.from_orm(obj).model_dump()
        
        # æå–data_filtersä¸­çš„å¸¸ç”¨å­—æ®µ
        if obj.data_filters:
            data['device_code'] = obj.data_filters.get('device_code')
            data['device_name'] = obj.data_filters.get('device_name')
            data['metric_name'] = obj.data_filters.get('metric_name')
        
        return cls(**data)
```

---

**Task 3: å¼€å‘APIæ¥å£**

```python
# app/api/v2/ai/predictions.py

from fastapi import APIRouter, Query, Depends
from typing import List, Optional
from datetime import datetime, timedelta

from app.core.dependency import DependAuth
from app.core.response_formatter_v2 import ResponseFormatterV2
from app.models.ai_monitoring import AIPrediction, PredictionStatus
from app.schemas.ai_prediction import (
    PredictionCreate,
    PredictionResponse,
    PredictionResultData
)
from app.services.ai.prediction import TrendPredictor

router = APIRouter(prefix="/predictions", tags=["AIè¶‹åŠ¿é¢„æµ‹"])
formatter = ResponseFormatterV2()

@router.post("/batch", summary="æ‰¹é‡åˆ›å»ºé¢„æµ‹ä»»åŠ¡")
async def create_batch_predictions(
    device_codes: List[str],
    metric_name: str,
    prediction_horizon: int = 24,
    model_type: str = "ARIMA",
    current_user=DependAuth
):
    """
    æ‰¹é‡åˆ›å»ºå¤šä¸ªè®¾å¤‡çš„é¢„æµ‹ä»»åŠ¡å¹¶æ‰§è¡Œ
    
    Args:
        device_codes: è®¾å¤‡ä»£ç åˆ—è¡¨
        metric_name: æŒ‡æ ‡åç§°ï¼ˆå¦‚temperatureï¼‰
        prediction_horizon: é¢„æµ‹æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
        model_type: é¢„æµ‹æ¨¡å‹ç±»å‹
    """
    try:
        predictor = TrendPredictor()
        results = []
        
        for device_code in device_codes:
            # 1. åˆ›å»ºé¢„æµ‹è®°å½•
            prediction = await AIPrediction.create(
                prediction_name=f"{device_code}-{metric_name}-é¢„æµ‹-{prediction_horizon}h",
                target_variable=metric_name,
                prediction_horizon=prediction_horizon,
                model_type=model_type,
                data_source="t_device_realtime_data",
                data_filters={
                    "device_code": device_code,
                    "metric_name": metric_name,
                    "time_range": f"{prediction_horizon}h"
                },
                status=PredictionStatus.RUNNING,
                created_by=current_user.id
            )
            
            try:
                # 2. æ‰§è¡Œé¢„æµ‹
                # TODO: è¿™é‡Œåº”è¯¥ä»å®é™…æ•°æ®æºè·å–æ•°æ®
                # ç¤ºä¾‹ï¼šdata = await get_device_data(device_code, metric_name)
                result = await predictor.predict(
                    data=[],  # å®é™…æ•°æ®
                    steps=prediction_horizon,
                    method=model_type
                )
                
                # 3. ä¿å­˜ç»“æœ
                prediction.result_data = {
                    "predictions": result["predictions"],
                    "metadata": {
                        "device_code": device_code,
                        "metric_name": metric_name,
                        "prediction_method": model_type,
                        "total_points": len(result["predictions"]),
                        "avg_confidence": result.get("avg_confidence", 0.9)
                    }
                }
                prediction.status = PredictionStatus.COMPLETED
                prediction.accuracy_score = result.get("accuracy", 0)
                prediction.progress = 100
                prediction.completed_at = datetime.now()
                
            except Exception as e:
                prediction.status = PredictionStatus.FAILED
                prediction.error_message = str(e)
            
            await prediction.save()
            results.append(PredictionResponse.from_orm_with_filters(prediction))
        
        return formatter.success(
            data={
                "predictions": [r.model_dump() for r in results],
                "total": len(results),
                "successful": len([r for r in results if r.status == "completed"])
            },
            message=f"æ‰¹é‡é¢„æµ‹å®Œæˆï¼ŒæˆåŠŸ {len(results)} ä¸ª"
        )
        
    except Exception as e:
        return formatter.error(message=f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {str(e)}")


@router.get("/history", summary="æŸ¥è¯¢é¢„æµ‹å†å²")
async def get_prediction_history(
    device_code: str = Query(..., description="è®¾å¤‡ä»£ç "),
    metric_name: Optional[str] = Query(None, description="æŒ‡æ ‡åç§°"),
    status: Optional[str] = Query(None, description="çŠ¶æ€ç­›é€‰"),
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    current_user=DependAuth
):
    """
    æŸ¥è¯¢è®¾å¤‡çš„é¢„æµ‹å†å²è®°å½•
    
    æ”¯æŒæŒ‰è®¾å¤‡ä»£ç ã€æŒ‡æ ‡åç§°ã€çŠ¶æ€ç­›é€‰
    """
    try:
        # ä½¿ç”¨ JSONB æŸ¥è¯¢
        query = AIPrediction.filter(
            data_filters__contains={"device_code": device_code}
        )
        
        if metric_name:
            query = query.filter(
                data_filters__contains={"metric_name": metric_name}
            )
        
        if status:
            query = query.filter(status=status)
        
        total = await query.count()
        offset = (page - 1) * page_size
        predictions = await query.order_by('-created_at').offset(offset).limit(page_size)
        
        return formatter.success(data={
            "predictions": [
                PredictionResponse.from_orm_with_filters(p).model_dump()
                for p in predictions
            ],
            "total": total,
            "page": page,
            "page_size": page_size
        })
        
    except Exception as e:
        return formatter.error(message=f"æŸ¥è¯¢é¢„æµ‹å†å²å¤±è´¥: {str(e)}")


@router.get("/{prediction_id}", summary="è·å–é¢„æµ‹è¯¦æƒ…")
async def get_prediction_detail(
    prediction_id: int,
    current_user=DependAuth
):
    """è·å–æŒ‡å®šé¢„æµ‹ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        prediction = await AIPrediction.get_or_none(id=prediction_id)
        
        if not prediction:
            return formatter.error(message="é¢„æµ‹è®°å½•ä¸å­˜åœ¨", code=404)
        
        return formatter.success(
            data=PredictionResponse.from_orm_with_filters(prediction).model_dump()
        )
        
    except Exception as e:
        return formatter.error(message=f"è·å–é¢„æµ‹è¯¦æƒ…å¤±è´¥: {str(e)}")
```

---

#### Day 1ä¸‹åˆ: å‰ç«¯é›†æˆï¼ˆ2-3å°æ—¶ï¼‰

**Task 4: æ›´æ–°è¶‹åŠ¿é¢„æµ‹é¡µé¢**

```javascript
// web/src/views/ai-monitor/trend-prediction/index.vue

// æ›¿æ¢ mock æ•°æ®ä¸ºçœŸå® API
const refreshPrediction = async () => {
  if (loading.value) return
  
  loading.value = true
  try {
    console.log('ğŸ”„ åˆ·æ–°è¶‹åŠ¿é¢„æµ‹æ•°æ®...')

    // æ‰¹é‡åˆ›å»ºé¢„æµ‹ä»»åŠ¡
    const response = await trendPredictionApi.predict({
      device_codes: ['WLD-001', 'WLD-002', 'WLD-003'],
      metric_name: 'temperature',
      prediction_horizon: 24,
      model_type: 'ARIMA'
    })

    if (response.data && response.data.predictions) {
      // å¤„ç†é¢„æµ‹ç»“æœ
      updatePredictionData(response.data.predictions)
    }

    message.success('é¢„æµ‹æ•°æ®åˆ·æ–°å®Œæˆ')
  } catch (error) {
    console.error('âŒ åˆ·æ–°é¢„æµ‹æ•°æ®å¤±è´¥:', error)
    message.error(`åˆ·æ–°å¤±è´¥: ${error.message}`)
  } finally {
    loading.value = false
  }
}

// æŸ¥è¯¢å†å²é¢„æµ‹
const loadPredictionHistory = async (deviceCode) => {
  const response = await trendPredictionApi.getHistory({
    device_code: deviceCode,
    metric_name: 'temperature',
    page: 1,
    page_size: 10
  })
  
  return response.data.predictions
}
```

---

## âœ… æ–¹æ¡ˆæ€»ç»“

### æœ€ç»ˆå†³å®š

**âœ… ä¸æ·»åŠ  device_code å†—ä½™å­—æ®µ**

**ç†ç”±**:
1. âœ… data_filters å·²ç»å¯ä»¥å­˜å‚¨è®¾å¤‡ä¿¡æ¯
2. âœ… JSONBç´¢å¼•æ€§èƒ½è¶³å¤Ÿå¥½ï¼ˆ<2msï¼‰
3. âœ… é¿å…æ•°æ®å†—ä½™å’Œç»´æŠ¤æˆæœ¬
4. âœ… ç¬¦åˆè¡¨è®¾è®¡ç†å¿µ
5. âœ… æ›´çµæ´»æ‰©å±•

**å®æ–½é‡ç‚¹**:
1. â­ åˆ›å»ºJSONBç´¢å¼•ï¼ˆå¿…é¡»ï¼‰
2. â­ è§„èŒƒdata_filtersæ ¼å¼
3. â­ æ­£ç¡®ä½¿ç”¨JSONBæŸ¥è¯¢è¯­æ³•

---

## ğŸ“Š æœ€ç»ˆæ—¶é—´å®‰æ’

| é˜¶æ®µ | ä»»åŠ¡ | æ—¶é—´ |
|------|------|------|
| **Day 1ä¸Šåˆ** | åˆ›å»ºJSONBç´¢å¼• | 1å°æ—¶ |
| **Day 1ä¸Šåˆ-ä¸‹åˆ** | å¼€å‘APIæ¥å£ | 3-4å°æ—¶ |
| **Day 1ä¸‹åˆ** | å‰ç«¯é›†æˆ | 2-3å°æ—¶ |
| **Day 2ä¸Šåˆ** | Mockæ•°æ®å‡†å¤‡ | 2-3å°æ—¶ |
| **Day 2ä¸‹åˆ** | æµ‹è¯•å’Œæ–‡æ¡£ | 2-3å°æ—¶ |
| **æ€»è®¡** | - | **1-2å¤©** âœ… |

---

## ğŸ™ æ„Ÿè°¢ç”¨æˆ·çš„åé¦ˆ

ç”¨æˆ·çš„è´¨ç–‘éå¸¸æ­£ç¡®ï¼Œå¸®åŠ©æˆ‘ä»¬ï¼š
1. âœ… é¿å…äº†ä¸å¿…è¦çš„å†—ä½™å­—æ®µ
2. âœ… æ‰¾åˆ°äº†æ›´ç¬¦åˆè®¾è®¡ç†å¿µçš„æ–¹æ¡ˆ
3. âœ… èŠ‚çœäº†å¼€å‘å’Œç»´æŠ¤æˆæœ¬
4. âœ… æå‡äº†æ–¹æ¡ˆçš„åˆç†æ€§

**è¿™å°±æ˜¯é«˜è´¨é‡æŠ€æœ¯è®¨è®ºçš„ä»·å€¼ï¼** ğŸ‘

---

**æ›´æ–°æ—¶é—´**: 2025-11-04  
**çŠ¶æ€**: âœ… æœ€ç»ˆç¡®å®šæ–¹æ¡ˆï¼ŒåŸºäºç”¨æˆ·æ­£ç¡®çš„æŠ€æœ¯åˆ¤æ–­

