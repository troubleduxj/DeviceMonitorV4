# Week 2-3: AIåŠŸèƒ½å¼€å‘è¯¦ç»†è®¡åˆ’

**è®¡åˆ’å‘¨æœŸ**: 2-3å‘¨  
**å¼€å§‹æ—¥æœŸ**: 2025-11-04  
**çŠ¶æ€**: å¾…å¼€å§‹  
**å‰ç½®æ¡ä»¶**: Week 1æ¨¡å—åŒ–å®æ–½å·²å®Œæˆ âœ…

---

## ğŸ“‹ æ€»ä½“ç›®æ ‡

åœ¨Week 1å»ºç«‹çš„æ¨¡å—åŒ–æ¶æ„åŸºç¡€ä¸Šï¼Œå®ç°ä»¥ä¸‹æ ¸å¿ƒAIåŠŸèƒ½ï¼š
1. **ç‰¹å¾æå–æœåŠ¡** - ä»è®¾å¤‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„ç‰¹å¾
2. **å¼‚å¸¸æ£€æµ‹ç®—æ³•** - å®æ—¶æ£€æµ‹è®¾å¤‡è¿è¡Œå¼‚å¸¸
3. **è¶‹åŠ¿é¢„æµ‹æ¨¡å‹** - é¢„æµ‹è®¾å¤‡çŠ¶æ€è¶‹åŠ¿
4. **å¥åº·è¯„åˆ†ç³»ç»Ÿ** - ç»¼åˆè¯„ä¼°è®¾å¤‡å¥åº·çŠ¶å†µ

---

## ğŸ—“ï¸ å®æ–½æ—¶é—´è¡¨

### Week 2: æ ¸å¿ƒç®—æ³•å®ç°ï¼ˆ7å¤©ï¼‰

#### Day 1-2: ç‰¹å¾æå–æœåŠ¡
- [ ] ç»Ÿè®¡ç‰¹å¾æå–
- [ ] æ—¶é—´åºåˆ—ç‰¹å¾
- [ ] é¢‘åŸŸç‰¹å¾
- [ ] ç‰¹å¾ç¼“å­˜æœºåˆ¶

#### Day 3-4: å¼‚å¸¸æ£€æµ‹ç®—æ³•
- [ ] åŸºäºç»Ÿè®¡çš„å¼‚å¸¸æ£€æµ‹ï¼ˆ3-sigmaè§„åˆ™ï¼‰
- [ ] åŸºäºå­¤ç«‹æ£®æ—çš„å¼‚å¸¸æ£€æµ‹
- [ ] å¼‚å¸¸ç­‰çº§åˆ’åˆ†
- [ ] å¼‚å¸¸è®°å½•å­˜å‚¨

#### Day 5-6: è¶‹åŠ¿é¢„æµ‹æ¨¡å‹
- [ ] ARIMAæ—¶é—´åºåˆ—é¢„æµ‹
- [ ] ç®€å•ç§»åŠ¨å¹³å‡é¢„æµ‹
- [ ] é¢„æµ‹ç»“æœç¼“å­˜
- [ ] é¢„æµ‹å‡†ç¡®åº¦è¯„ä¼°

#### Day 7: å¥åº·è¯„åˆ†ç³»ç»Ÿ
- [ ] å¤šç»´åº¦æŒ‡æ ‡å®šä¹‰
- [ ] ç»¼åˆè¯„åˆ†ç®—æ³•
- [ ] å¥åº·ç­‰çº§åˆ’åˆ†
- [ ] è¯„åˆ†å†å²è®°å½•

### Week 3: é›†æˆä¸ä¼˜åŒ–ï¼ˆ7-10å¤©ï¼‰

#### Day 1-2: APIæ¥å£å®ç°
- [ ] ç‰¹å¾æå–API
- [ ] å¼‚å¸¸æ£€æµ‹API
- [ ] è¶‹åŠ¿é¢„æµ‹API
- [ ] å¥åº·è¯„åˆ†API

#### Day 3-4: å‰ç«¯é›†æˆ
- [ ] AIä»ªè¡¨ç›˜é¡µé¢
- [ ] å¼‚å¸¸æ£€æµ‹é¡µé¢
- [ ] è¶‹åŠ¿é¢„æµ‹é¡µé¢
- [ ] å¥åº·è¯„åˆ†é¡µé¢

#### Day 5-6: æ€§èƒ½ä¼˜åŒ–
- [ ] ç®—æ³•æ€§èƒ½ä¼˜åŒ–
- [ ] ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
- [ ] å¼‚æ­¥ä»»åŠ¡å¤„ç†
- [ ] èµ„æºä½¿ç”¨ä¼˜åŒ–

#### Day 7: æµ‹è¯•ä¸æ–‡æ¡£
- [ ] å•å…ƒæµ‹è¯•
- [ ] é›†æˆæµ‹è¯•
- [ ] APIæ–‡æ¡£
- [ ] ç”¨æˆ·æ–‡æ¡£

---

## ğŸ¯ è¯¦ç»†ä»»åŠ¡åˆ†è§£

### 1. ç‰¹å¾æå–æœåŠ¡

#### 1.1 ç»Ÿè®¡ç‰¹å¾
**æ–‡ä»¶**: `app/services/ai/feature_extraction.py`

**åŠŸèƒ½**:
- å‡å€¼ã€æ–¹å·®ã€æ ‡å‡†å·®
- æœ€å¤§å€¼ã€æœ€å°å€¼ã€èŒƒå›´
- åˆ†ä½æ•°ï¼ˆ25%, 50%, 75%ï¼‰
- å³°åº¦ã€ååº¦

**å®ç°æ€è·¯**:
```python
class StatisticalFeatureExtractor:
    def extract(self, data: List[float]) -> Dict[str, float]:
        return {
            'mean': np.mean(data),
            'std': np.std(data),
            'max': np.max(data),
            'min': np.min(data),
            'q25': np.percentile(data, 25),
            'q50': np.percentile(data, 50),
            'q75': np.percentile(data, 75),
        }
```

#### 1.2 æ—¶é—´åºåˆ—ç‰¹å¾
**åŠŸèƒ½**:
- è¶‹åŠ¿ï¼ˆä¸Šå‡/ä¸‹é™/å¹³ç¨³ï¼‰
- å‘¨æœŸæ€§æ£€æµ‹
- è‡ªç›¸å…³ç³»æ•°
- å˜åŒ–ç‡

**å®ç°æ€è·¯**:
```python
class TimeSeriesFeatureExtractor:
    def extract_trend(self, data: List[float]) -> str:
        """æå–è¶‹åŠ¿ï¼šä¸Šå‡ã€ä¸‹é™ã€å¹³ç¨³"""
        coefficients = np.polyfit(range(len(data)), data, 1)
        slope = coefficients[0]
        if slope > 0.1:
            return "ä¸Šå‡"
        elif slope < -0.1:
            return "ä¸‹é™"
        else:
            return "å¹³ç¨³"
```

#### 1.3 é¢‘åŸŸç‰¹å¾
**åŠŸèƒ½**:
- FFTé¢‘è°±åˆ†æ
- ä¸»é¢‘ç‡è¯†åˆ«
- é¢‘ç‡èƒ½é‡åˆ†å¸ƒ

**å®ç°æ€è·¯**:
```python
class FrequencyFeatureExtractor:
    def extract(self, data: List[float]) -> Dict:
        fft_result = np.fft.fft(data)
        frequencies = np.fft.fftfreq(len(data))
        # æå–ä¸»é¢‘ç‡ã€èƒ½é‡ç­‰
```

---

### 2. å¼‚å¸¸æ£€æµ‹ç®—æ³•

#### 2.1 åŸºäºç»Ÿè®¡çš„å¼‚å¸¸æ£€æµ‹
**æ–‡ä»¶**: `app/services/ai/anomaly_detection.py`

**æ–¹æ³•**: 3-sigmaè§„åˆ™
```python
class StatisticalAnomalyDetector:
    def detect(self, data: List[float]) -> List[Dict]:
        mean = np.mean(data)
        std = np.std(data)
        threshold = 3 * std
        
        anomalies = []
        for i, value in enumerate(data):
            if abs(value - mean) > threshold:
                anomalies.append({
                    'index': i,
                    'value': value,
                    'severity': self._calculate_severity(value, mean, std)
                })
        return anomalies
```

**å¼‚å¸¸ç­‰çº§**:
- è½»å¾®: 3Ïƒ < |x - Î¼| < 4Ïƒ
- ä¸­ç­‰: 4Ïƒ < |x - Î¼| < 5Ïƒ
- ä¸¥é‡: |x - Î¼| > 5Ïƒ

#### 2.2 åŸºäºå­¤ç«‹æ£®æ—çš„å¼‚å¸¸æ£€æµ‹
**æ–¹æ³•**: Isolation Forest
```python
from sklearn.ensemble import IsolationForest

class MLAnomalyDetector:
    def __init__(self):
        self.model = IsolationForest(contamination=0.1)
    
    def fit_predict(self, data: np.ndarray) -> np.ndarray:
        predictions = self.model.fit_predict(data)
        # -1è¡¨ç¤ºå¼‚å¸¸ï¼Œ1è¡¨ç¤ºæ­£å¸¸
        return predictions
```

#### 2.3 å¼‚å¸¸è®°å½•å­˜å‚¨
**æ•°æ®åº“è¡¨**: `t_ai_anomaly_record`
```sql
CREATE TABLE t_ai_anomaly_record (
    id SERIAL PRIMARY KEY,
    device_id VARCHAR(50),
    metric_name VARCHAR(100),
    anomaly_value FLOAT,
    expected_value FLOAT,
    severity VARCHAR(20),
    detection_method VARCHAR(50),
    detected_at TIMESTAMP,
    status VARCHAR(20) DEFAULT 'active'
);
```

---

### 3. è¶‹åŠ¿é¢„æµ‹æ¨¡å‹

#### 3.1 ARIMAé¢„æµ‹
**æ–‡ä»¶**: `app/services/ai/prediction.py`

```python
from statsmodels.tsa.arima.model import ARIMA

class ARIMAPredictor:
    def predict(self, history: List[float], steps: int = 10) -> List[float]:
        """
        ä½¿ç”¨ARIMAæ¨¡å‹é¢„æµ‹æœªæ¥å€¼
        
        Args:
            history: å†å²æ•°æ®
            steps: é¢„æµ‹æ­¥æ•°
        
        Returns:
            é¢„æµ‹å€¼åˆ—è¡¨
        """
        model = ARIMA(history, order=(1, 1, 1))
        model_fit = model.fit()
        forecast = model_fit.forecast(steps=steps)
        return forecast.tolist()
```

#### 3.2 ç®€å•ç§»åŠ¨å¹³å‡é¢„æµ‹
```python
class MovingAveragePredictor:
    def predict(self, history: List[float], window: int = 5) -> float:
        """ç®€å•ç§»åŠ¨å¹³å‡é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼"""
        if len(history) < window:
            return np.mean(history)
        return np.mean(history[-window:])
```

#### 3.3 é¢„æµ‹å‡†ç¡®åº¦è¯„ä¼°
**æŒ‡æ ‡**:
- MAE (å¹³å‡ç»å¯¹è¯¯å·®)
- RMSE (å‡æ–¹æ ¹è¯¯å·®)
- MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®)

```python
class PredictionEvaluator:
    @staticmethod
    def calculate_mae(actual: List[float], predicted: List[float]) -> float:
        return np.mean(np.abs(np.array(actual) - np.array(predicted)))
    
    @staticmethod
    def calculate_rmse(actual: List[float], predicted: List[float]) -> float:
        return np.sqrt(np.mean((np.array(actual) - np.array(predicted)) ** 2))
```

---

### 4. å¥åº·è¯„åˆ†ç³»ç»Ÿ

#### 4.1 è¯„åˆ†ç»´åº¦
**æ–‡ä»¶**: `app/services/ai/health_scoring.py`

**ç»´åº¦å®šä¹‰**:
1. **æ€§èƒ½æŒ‡æ ‡** (30%): åŸºäºå…³é”®æ€§èƒ½å‚æ•°
2. **å¼‚å¸¸é¢‘ç‡** (25%): æœ€è¿‘å¼‚å¸¸å‘ç”Ÿé¢‘ç‡
3. **è¶‹åŠ¿å¥åº·** (25%): è¶‹åŠ¿æ˜¯å¦å¥åº·
4. **è¿è¡Œæ—¶é•¿** (20%): è¿ç»­è¿è¡Œç¨³å®šæ€§

```python
class HealthScoreCalculator:
    WEIGHTS = {
        'performance': 0.30,
        'anomaly_frequency': 0.25,
        'trend': 0.25,
        'uptime': 0.20
    }
    
    def calculate(self, metrics: Dict) -> Dict:
        """è®¡ç®—ç»¼åˆå¥åº·è¯„åˆ†"""
        scores = {
            'performance': self._score_performance(metrics),
            'anomaly_frequency': self._score_anomaly(metrics),
            'trend': self._score_trend(metrics),
            'uptime': self._score_uptime(metrics)
        }
        
        # åŠ æƒå¹³å‡
        total_score = sum(
            scores[key] * self.WEIGHTS[key] 
            for key in scores
        )
        
        return {
            'total_score': total_score,
            'grade': self._get_grade(total_score),
            'details': scores
        }
    
    def _get_grade(self, score: float) -> str:
        """è¯„åˆ†ç­‰çº§"""
        if score >= 90: return 'A-ä¼˜ç§€'
        elif score >= 80: return 'B-è‰¯å¥½'
        elif score >= 70: return 'C-ä¸€èˆ¬'
        elif score >= 60: return 'D-è¾ƒå·®'
        else: return 'F-å±é™©'
```

#### 4.2 è¯„åˆ†å†å²è®°å½•
**æ•°æ®åº“è¡¨**: `t_ai_health_score`
```sql
CREATE TABLE t_ai_health_score (
    id SERIAL PRIMARY KEY,
    device_id VARCHAR(50),
    total_score FLOAT,
    grade VARCHAR(20),
    performance_score FLOAT,
    anomaly_score FLOAT,
    trend_score FLOAT,
    uptime_score FLOAT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## ğŸ”Œ APIæ¥å£è®¾è®¡

### ç‰¹å¾æå–API
```python
@router.post("/ai/features/extract")
@require_ai_feature("feature_extraction")
async def extract_features(
    device_id: str,
    metric_name: str,
    time_range: Optional[str] = "1h"
):
    """æå–è®¾å¤‡æŒ‡æ ‡ç‰¹å¾"""
    pass
```

### å¼‚å¸¸æ£€æµ‹API
```python
@router.post("/ai/anomaly/detect")
@require_ai_feature("anomaly_detection")
async def detect_anomalies(
    device_id: str,
    metric_name: str,
    method: str = "statistical"  # or "ml"
):
    """æ£€æµ‹å¼‚å¸¸"""
    pass

@router.get("/ai/anomaly/list")
@require_ai_feature("anomaly_detection")
async def list_anomalies(
    device_id: Optional[str] = None,
    severity: Optional[str] = None,
    page: int = 1,
    page_size: int = 20
):
    """æŸ¥è¯¢å¼‚å¸¸è®°å½•"""
    pass
```

### è¶‹åŠ¿é¢„æµ‹API
```python
@router.post("/ai/prediction/forecast")
@require_ai_feature("trend_prediction")
async def forecast_trend(
    device_id: str,
    metric_name: str,
    steps: int = 10,
    model: str = "arima"  # or "ma"
):
    """è¶‹åŠ¿é¢„æµ‹"""
    pass
```

### å¥åº·è¯„åˆ†API
```python
@router.get("/ai/health/score/{device_id}")
@require_ai_feature("health_scoring")
async def get_health_score(device_id: str):
    """è·å–è®¾å¤‡å¥åº·è¯„åˆ†"""
    pass

@router.get("/ai/health/history/{device_id}")
@require_ai_feature("health_scoring")
async def get_health_history(
    device_id: str,
    days: int = 7
):
    """è·å–å¥åº·è¯„åˆ†å†å²"""
    pass
```

---

## ğŸ“¦ ä¾èµ–åŒ…

### æ–°å¢PythonåŒ…
```txt
# æ•°æ®åˆ†æ
numpy>=1.21.0
pandas>=1.3.0
scipy>=1.7.0

# æœºå™¨å­¦ä¹ 
scikit-learn>=1.0.0

# æ—¶é—´åºåˆ—
statsmodels>=0.13.0

# æ€§èƒ½ä¼˜åŒ–
numba>=0.54.0  # å¯é€‰ï¼Œç”¨äºåŠ é€Ÿ
```

### å®‰è£…å‘½ä»¤
```bash
pip install numpy pandas scipy scikit-learn statsmodels
```

---

## âœ… éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] æ‰€æœ‰æ ¸å¿ƒç®—æ³•å®ç°å¹¶é€šè¿‡å•å…ƒæµ‹è¯•
- [ ] APIæ¥å£å®Œæ•´ä¸”æ–‡æ¡£æ¸…æ™°
- [ ] å‰ç«¯é¡µé¢å¯ç”¨ä¸”æ•°æ®å±•ç¤ºæ­£ç¡®
- [ ] æ€§èƒ½æ»¡è¶³è¦æ±‚ï¼ˆå“åº”æ—¶é—´<2sï¼‰

### è´¨é‡éªŒæ”¶
- [ ] ä»£ç æµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%
- [ ] æ— ä¸¥é‡æ€§èƒ½é—®é¢˜
- [ ] æ— ä¸¥é‡bug
- [ ] æ–‡æ¡£å®Œæ•´

### æ€§èƒ½æŒ‡æ ‡
- ç‰¹å¾æå–: <500ms (1000æ¡æ•°æ®)
- å¼‚å¸¸æ£€æµ‹: <1s (1000æ¡æ•°æ®)
- è¶‹åŠ¿é¢„æµ‹: <2s (100æ¡å†å²æ•°æ®ï¼Œé¢„æµ‹10æ­¥)
- å¥åº·è¯„åˆ†: <300ms

---

## ğŸ“ æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•
- ç‰¹å¾æå–å™¨æµ‹è¯•
- å¼‚å¸¸æ£€æµ‹å™¨æµ‹è¯•
- é¢„æµ‹æ¨¡å‹æµ‹è¯•
- å¥åº·è¯„åˆ†æµ‹è¯•

### é›†æˆæµ‹è¯•
- APIç«¯åˆ°ç«¯æµ‹è¯•
- å‰åç«¯é›†æˆæµ‹è¯•
- æ€§èƒ½å‹åŠ›æµ‹è¯•

### æµ‹è¯•æ•°æ®
- ä½¿ç”¨æ¨¡æ‹Ÿè®¾å¤‡æ•°æ®
- è¦†ç›–æ­£å¸¸ã€å¼‚å¸¸ã€è¾¹ç•Œæƒ…å†µ
- ä¸åŒæ•°æ®é‡æµ‹è¯•ï¼ˆ10æ¡ã€100æ¡ã€1000æ¡ï¼‰

---

## ğŸš€ éƒ¨ç½²è®¡åˆ’

### å¼€å‘ç¯å¢ƒ
1. å®‰è£…ä¾èµ–åŒ…
2. é…ç½®AIåŠŸèƒ½å¼€å…³
3. è¿è¡Œæµ‹è¯•éªŒè¯

### ç”Ÿäº§ç¯å¢ƒ
1. æ€§èƒ½æµ‹è¯•é€šè¿‡å
2. åˆ†é˜¶æ®µå¯ç”¨åŠŸèƒ½
3. ç›‘æ§èµ„æºä½¿ç”¨
4. æ”¶é›†ç”¨æˆ·åé¦ˆ

---

## ğŸ“Š é£é™©è¯„ä¼°

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|----------|
| ç®—æ³•æ€§èƒ½ä¸è¶³ | é«˜ | ä¸­ | æå‰æ€§èƒ½æµ‹è¯•ï¼Œä¼˜åŒ–ç®—æ³• |
| ä¾èµ–åŒ…å…¼å®¹æ€§ | ä¸­ | ä½ | é”å®šç‰ˆæœ¬ï¼Œå……åˆ†æµ‹è¯• |
| æ•°æ®è´¨é‡é—®é¢˜ | é«˜ | ä¸­ | æ•°æ®é¢„å¤„ç†ï¼Œå¼‚å¸¸å¤„ç† |
| èµ„æºå ç”¨è¿‡é«˜ | ä¸­ | ä¸­ | èµ„æºç›‘æ§ï¼Œé™æµæ§åˆ¶ |

---

## ğŸ“š å‚è€ƒèµ„æ–™

### ç®—æ³•æ–‡æ¡£
- ARIMA: https://otexts.com/fpp2/arima.html
- Isolation Forest: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html
- æ—¶é—´åºåˆ—åˆ†æ: https://www.statsmodels.org/stable/tsa.html

### æœ€ä½³å®è·µ
- ç‰¹å¾å·¥ç¨‹: https://www.featuretools.com/
- å¼‚å¸¸æ£€æµ‹: https://scikit-learn.org/stable/modules/outlier_detection.html

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³å¼€å§‹**: Day 1-2 ç‰¹å¾æå–æœåŠ¡å®ç°

**å…³é”®é‡Œç¨‹ç¢‘**:
- Week 2 Day 7: æ ¸å¿ƒç®—æ³•å®Œæˆ
- Week 3 Day 4: å‰ç«¯é›†æˆå®Œæˆ
- Week 3 Day 7: å…¨éƒ¨å®Œæˆå¹¶äº¤ä»˜

---

**è®¡åˆ’åˆ¶å®šæ—¥æœŸ**: 2025-11-04  
**è®¡åˆ’åˆ¶å®šè€…**: AI Assistant  
**å®¡æ ¸çŠ¶æ€**: å¾…ç¡®è®¤

